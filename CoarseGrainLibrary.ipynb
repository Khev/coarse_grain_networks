{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "                                                   ## FUNCTIONS FOR INITIALISING GRAPHS ##\n",
      "\"\"\"\n",
      "    Note: Below is the full collection of algorithms I developed. There may be some inconsistencies, since\n",
      "    a) as I grew more accustomed to python, my notation changed. b) As the project developed, I refined / \n",
      "    rewrote entirely / deleted, some routines to be more efficient / as they became redundant.\n",
      "\n",
      "    I decided to lump them all into one big .py to make calling them easier. The functions are roughly listed\n",
      "    in chronological order (the order in which i developled them).\n",
      "\n",
      "    I still haven't made use of .logger\n",
      "\n",
      "\"\"\"    \n",
      "    \n",
      "    \n",
      "import cPickle as pickle\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import time\n",
      "import cProfile        \n",
      "#import pycallgraph    #for profiling\n",
      "#import pydot          #for profiling\n",
      "import math\n",
      "import Image\n",
      "import random\n",
      "import scipy.sparse as ss\n",
      "\n",
      "\n",
      "#For profiling\n",
      "def profile(string):\n",
      "    \"\"\" Time profiler.\n",
      "\n",
      "        Input: string as 'f(arg1, arg2, ...)'\n",
      "        Ouput: produces time stats and Bottleneck.png\n",
      "    \"\"\"\n",
      "    \n",
      "    pycallgraph.start_trace()\n",
      "    cProfile.run(string)\n",
      "    pycallgraph.make_dot_graph('BottleNeck.png')\n",
      "    \n",
      "    \n",
      "    \n",
      "def compare(function1, function2, *args):\n",
      "    \"\"\" Returns the runtime of two functions\n",
      "    \"\"\"\n",
      "    with Timer():\n",
      "        function1(*args)\n",
      "    with Timer():\n",
      "        function2(*args)\n",
      "    \n",
      "\n",
      "#Dictionary of landuse types -- NOT exhaustive (I didn't include some of the more obscure data type)\n",
      "uses = {'Crop Barley/Soy Beans': 254, \n",
      "'Crop Corn/Soy Beans': 241, \n",
      "'Crop Soybeans/Oats': 240, \n",
      "'Crop Soybeans / Cotton': 239,\n",
      "'Crop WinterWheat/Cotton': 238,\n",
      "'Crop Barely/Corn': 237,\n",
      "'Crop Winter Wheat/Sorghum': 236,\n",
      "'Crop Barely / Sorghum': 235,\n",
      "'Crop Durum Wheat/ Sorghum': 234,\n",
      "'Crop Lettuce/Barely': 233,\n",
      "'Crop Lettuce/Cotton': 232,\n",
      "'Crop Lettuce / Cantaloupe': 231,\n",
      "'Crop Lettuce/Durum Wheat': 230,\n",
      "'Pumpkins': 229,\n",
      "'Lettuce': 227,\n",
      "'Crop Oats/Corn': 226,\n",
      "'Crop Winter Wheat / Corn': 225,\n",
      "'Vetch': 224,\n",
      "'Greens': 219,\n",
      "'Clouds / No data': 81,\n",
      "'Shrubland': 64,\n",
      "'Fallow / Idle Cropland': 61,\n",
      "'Sod / Grass Seed': 59,\n",
      "'Herbs': 57,\n",
      "'Hops': 56,\n",
      "'Misc Vegetables': 47,\n",
      "'Other Crops': 44,\n",
      "'Buck Wheat': 39,\n",
      "'Oats': 28,\n",
      "'Rye': 27,\n",
      "'Crop Winter Wheat / Soybeans': 26,\n",
      "'Other Small Grains': 25,\n",
      "'Winter Wheat': 24,\n",
      "'Spring Wheat': 23,\n",
      "'Durum Wheat': 22,\n",
      "'Barely': 21,\n",
      "'Soybeans': 5,\n",
      "'Corn': 1,\n",
      "'Water': 83,\n",
      "'Open Water': 111\n",
      "    \n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "class Timer():\n",
      "   def __enter__(self): self.start = time.time()\n",
      "   def __exit__(self, *args): print time.time() - self.start\n",
      "\n",
      "            \n",
      "def makeGraph(data):\n",
      "    \n",
      "    \"\"\" Makes a graph from [(x1,y1), ..], as stored in XWheatResults.\n",
      "        Each node is labeled by a string of form 'x1 y1'. So, g['x1 y1'] return the node at (x1, y1).\n",
      "    \"\"\"\n",
      "\n",
      "    g = nx.Graph()\n",
      "    for i in xrange(len(data)):\n",
      "        g.add_node(str(data[i][0]) + ' ' + str(data[i][1]))\n",
      "    return g\n",
      "\n",
      "\n",
      "\n",
      "def dist(s1, s2):\n",
      "    \n",
      "    \"\"\"Returns dist between to nodes, input: ('x1 y1', 'x2, y2'), i.e. as strings. \"\"\"\n",
      "        \n",
      "    temp1 = np.array([int(x) for x in s1.split()])\n",
      "    temp2 = np.array([int(x) for x in s2.split()])\n",
      "    \n",
      "    return math.hypot(temp1[0] - temp2[0], temp1[1] - temp2[1])\n",
      "\n",
      "   \n",
      "    \n",
      "#Returns array of positions, [[x1, y1], ...], of nodes in a graph g.\n",
      "def listPositions(g):\n",
      "    \n",
      "    \"\"\" Returns array of positions, [[x1, y1], ...], of nodes in a graph g. \"\"\"\n",
      "\n",
      "    pos = [key for key in g.node]                  #list of positions as strings\n",
      "    for i in xrange(len(g.node)):\n",
      "        pos[i] = [int(x) for x in pos[i].split()]  #list of positions as [i,j]\n",
      "    return pos\n",
      "\n",
      "\n",
      "\n",
      "#plots nodes of graph as a scatter plot.\n",
      "def plotGraph(g):\n",
      "    \n",
      "    \"\"\" Plots nodes of graph as a scatter plot. \"\"\"\n",
      "\n",
      "    \n",
      "    temp = np.array(listPositions(g))\n",
      "    plt.scatter(temp[:,0], temp[:,1])\n",
      "    \n",
      "    \n",
      "    \n",
      "def string_to_list(string):\n",
      "    return [int(x) for x in string.split()]\n",
      "\n",
      "\n",
      "\n",
      "def subplot(node1, node2):\n",
      "    \"\"\"Intermediary Function for plot_graph_with_edges\"\"\"\n",
      "    \n",
      "    p1 = string_to_list(node1)\n",
      "    p2 = string_to_list(node2)\n",
      "    \n",
      "    temp = zip(p1, p2)\n",
      "\n",
      "    plt.plot(temp[0], temp[1], 'r', zorder = 1, lw = 1)\n",
      "    plt.scatter(temp[0], temp[1], s=60, zorder = 2)\n",
      "    \n",
      "    \n",
      "    \n",
      "def plot_node_with_neighbors(node, g):\n",
      "    for j in g[node]:\n",
      "        subplot(node, j)\n",
      "    \n",
      "    temp = string_to_list(node)\n",
      "    plt.scatter(temp[0], temp[1], c = 'r', s = 160)\n",
      "    \n",
      "    \n",
      "    \n",
      "def plot_graph_with_edges(g):\n",
      "    \"\"\"Plots networkz graph g as a scatter plot, with\n",
      "       with edges joined in. Works best on complete grahps\n",
      "       that is, ones with one fully connected component. \n",
      "    \"\"\"\n",
      "    \n",
      "    for i in g.nodes():\n",
      "        for j in g[i]:      #g[node] = neighbours of node\n",
      "            subplot(i, j)\n",
      "    \n",
      "\n",
      "    \n",
      "def connect(g, R):            \n",
      "    \n",
      "    \"\"\" #Connects node_i to node_j iff dist(node_i, node_j) <= R, for all node_i in\n",
      "        the networkx graph g. \"\"\"\n",
      "    \n",
      "    pos = listPositions(g)\n",
      "    \n",
      "    \n",
      "    #Sum (i,0,N), (j,0,N), i<j\n",
      "    for i in xrange(len(pos)):\n",
      "        for j in xrange(i+1, len(pos)):\n",
      "            if math.hypot(pos[i][0] - pos[j][0], pos[i][1] - pos[j][1]) <= R:\n",
      "                g.add_edge(str(pos[i][0]) + ' ' + str(pos[i][1]), str(pos[j][0]) + ' ' + str(pos[j][1]))   #convert [x1, y1] -> 'x1 y1'\n",
      "            \n",
      "                \n",
      "                \n",
      "#------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                   ## FUNCTIONS FOR COARSE GRAINING ##\n",
      "#data of form [[x1, y1], [x2, y2], ...]                \n",
      "def probeLengthScale(data, r_min, r_max):\n",
      "    \"\"\"Investigates how qualitative behaviour (the number of connected components: CC = Connected Component, #CC = number of CC) of graph changes as a function of \n",
      "       interaction radius. Computes the number of CC for each r, denoted CC(r), in the interval (r_min, r_max),\n",
      "       and if CC(r_i) != CC(r_min), stops and prints r_i. I developed this to test coarse graining. Best to use this on a subset of data (I took the first 500 from).\n",
      "\n",
      "       data of form [[x1, y1], [x2, y2], ...]                \n",
      "\n",
      "    \"\"\"\n",
      "    \n",
      "    g_coarse = makeGraph(data)\n",
      "    connect(g_coarse, r_min)\n",
      "    \n",
      "    for i in xrange(r_min, r_max):\n",
      "        g = makeGraph(data)\n",
      "        connect(g, i)\n",
      "        if nx.connected_components(g_coarse) != nx.connected_components(g):\n",
      "            print 'Different # of connected components at r =  ' + str(i)\n",
      "            break\n",
      "            \n",
      "            \n",
      "#Input: nx.Graph(). Output: list of nodes (one from each CC)\n",
      "def mostConnectedNodes(g):\n",
      "    \"Finds which node has highest degree in each CC\"\n",
      "    \n",
      "    connected_components = nx.connected_components(g)\n",
      "    return [max(g.degree(i).iteritems(), key = lambda x:x[1])[0] for i in connected_components ]\n",
      "\n",
      "\n",
      "\n",
      "#Input: n = node, connected_components = list of connected components, (CC)\n",
      "def getNeighbors(n, connected_components):\n",
      "    \n",
      "    \"\"\"#Input: n = node, connected_components = list of connected components, (CC) \"\"\"\n",
      "    \n",
      "    return [i for i in connected_components if n in i]\n",
      "\n",
      "\n",
      "\n",
      "#Input: most_connected = list of nodes with highest degree, one from each CC\n",
      "def findBiggestNeighbor(n, connected_components, most_connected):\n",
      "    \n",
      "    \"\"\"Biggest = node with highest degree.\n",
      "       Input: most_connected = list of nodes with highest degree, one from each CC  \"\"\"\n",
      "    \n",
      "    return [most_connected[i] for i in xrange(len(connected_components)) if n in connected_components[i]]\n",
      "\n",
      "\n",
      "\n",
      "#Input: rips reduce CC, original CC                                \n",
      "def findBiggestExternalNode(n, smallCC, bigCC, most_connected_gg):\n",
      "    \n",
      "    \"\"\"#Input: rips reduce CC, original CC \"\"\"                         \n",
      "    \n",
      "    nearNeighbors = getNeighbors(n, smallCC)  \n",
      "    farNeighbors = getNeighbors(n, bigCC)\n",
      "    \n",
      "    if nearNeighbors != farNeighbors:\n",
      "        uncommonNeighbors = list(set(farNeighbors[0]) - set(nearNeighbors[0]))  #taking [0] deals with overbracketing issues\n",
      "        \n",
      "        temp = []\n",
      "        for i in uncommonNeighbors: \n",
      "            temp.append(findBiggestNeighbor(i, smallCC, most_connected_gg)[0])\n",
      "            temp = list(set(temp))\n",
      "            \n",
      "        return temp\n",
      "    \n",
      "    else:\n",
      "        return 0   #if no neighbours, return 0. Is this bad practice?\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "def connectCoarseGraph(most_connected_gg, radius_of_infection, g_coarse):\n",
      "    \"\"\"Connectes nodes in g_coarse if they were connected in g_original \"\"\"\n",
      "    \n",
      "    for i in xrange(len(most_connected_gg)):\n",
      "        for j in xrange(i+1, len(most_connected_gg)):\n",
      "                if dist(most_connected_gg[i], most_connected_gg[j]) <= radius_of_infection:  \n",
      "                #if not most_connected_gg[i] in g[most_connected_gg[j]]:                     #check if node i was unconnected to node j in graph g\n",
      "                    g_coarse.add_edge(most_connected_gg[i], most_connected_gg[j])\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "def connectNeighborsNeigbors(most_connected_gg, connected_components_gg, connected_components_g, g_coarse, g):\n",
      "    \"\"\"Connects nodes in g_coarse, if they had neighbors who were connected, and weren't connected themselves --> if the sub-clusters were connected. \"\"\"\n",
      "    \n",
      "    for i in xrange(len(connected_components_gg)):\n",
      "        \n",
      "        big_node = most_connected_gg[i]                                                #this is the node with highest degree, in each CC\n",
      "        small_nodes = connected_components_gg[i][0:len(connected_components_gg[i])]    #this is a list of big_nodes neighbors. Strange notation, l = list[0:end], \n",
      "        small_nodes.remove(big_node)                                                   #since python treats lists by reference                                                   \n",
      "    \n",
      "        for j in small_nodes:\n",
      "            # If any small node has a neighbour that the big node isn't connected to (unconnected Neighbor), connect the big node to this neighbour.\n",
      "            unconnectedNeighbor = findBiggestExternalNode(j, connected_components_gg, connected_components_g, most_connected_gg)\n",
      "            \n",
      "            for k in unconnectedNeighbor:\n",
      "                if not k == 0:                                            #0 means there is no neighbor, so if there IS a neighbour.\n",
      "                    if not big_node in g_coarse[k]:                              # in big_node wasn't originally (in original graph) connected to\n",
      "                #if not isConnected(unconnectedNeighbor[0], big_node, g):                neighbours neighbours, connect them.\n",
      "                        g_coarse.add_edge(big_node, k)\n",
      "                    \n",
      "                    \n",
      "                    \n",
      "                    \n",
      "def convexHull(points):\n",
      "    \"\"\"Computes the convex hull of a set of 2D points.\n",
      " \n",
      "    Input: an iterable sequence of (x, y) pairs representing the points.\n",
      "    Output: a list of vertices of the convex hull in counter-clockwise order,\n",
      "      starting from the vertex with the lexicographically smallest coordinates.\n",
      "    Implements Andrew's monotone chain algorithm. O(n log n) complexity.\n",
      "    \"\"\"\n",
      " \n",
      "    # Sort the points lexicographically (tuples are compared lexicographically).\n",
      "    # Remove duplicates to detect the case we have just one unique point.\n",
      "    points = sorted(set(points))\n",
      " \n",
      "    # Boring case: no points or a single point, possibly repeated multiple times.\n",
      "    if len(points) <= 1:\n",
      "        return points\n",
      " \n",
      "    # 2D cross product of OA and OB vectors, i.e. z-component of their 3D cross product.\n",
      "    # Returns a positive value, if OAB makes a counter-clockwise turn,\n",
      "    # negative for clockwise turn, and zero if the points are collinear.\n",
      "    def cross(o, a, b):\n",
      "        return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\n",
      " \n",
      "    # Build lower hull \n",
      "    lower = []\n",
      "    for p in points:\n",
      "        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\n",
      "            lower.pop()\n",
      "        lower.append(p)\n",
      " \n",
      "    # Build upper hull\n",
      "    upper = []\n",
      "    for p in reversed(points):\n",
      "        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\n",
      "            upper.pop()\n",
      "        upper.append(p)\n",
      " \n",
      "    # Concatenation of the lower and upper hulls gives the convex hull.\n",
      "    # Last point of each list is omitted because it is repeated at the beginning of the other list. \n",
      "    return lower[:-1] + upper[:-1]\n",
      "\n",
      "\n",
      "\n",
      "def areClustersConnected(boundary1, boundary2, radius_of_infection):\n",
      "    \"\"\" Checks if Cluster (connected_component) 1, 2, having boundary 1, 2, are within radius of infection.\n",
      "        boundaryI is a list of lists: { boundary_of_connected_component_i  }      \"\"\"\n",
      "    \n",
      "    temp = False\n",
      "    \n",
      "    for i in xrange(len(boundary1)):\n",
      "        for j in xrange(len(boundary2)):\n",
      "            if math.hypot(boundary1[i][0] - boundary2[j][0], boundary1[i][1] - boundary2[j][1]) <= radius_of_infection:\n",
      "                temp = True\n",
      "                break\n",
      "    return temp\n",
      "\n",
      "\n",
      "            \n",
      "def stringsToTuples(connected_components):\n",
      "    \"\"\" Change data structure \"\"\"\n",
      "    \n",
      "    temp = []\n",
      "    for l in connected_components:    \n",
      "      temp.append([tuple([int(x) for x in i.split()]) for i in l])\n",
      "    \n",
      "    return temp\n",
      "\n",
      "\n",
      "            \n",
      "def findBoundaries(connected_components):\n",
      "    positions = stringsToTuples(connected_components)\n",
      "    return [convexHull(i) for i in positions]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def connectClusters(connected_components_gg, g_coarse, most_connected_gg, radius_of_infection):\n",
      "    \"\"\"Connected Clusters if they are within the radius of infection \"\"\"\n",
      "    \n",
      "    boundaries = findBoundaries(connected_components_gg)\n",
      "    \n",
      "    for i in xrange(len(boundaries)):\n",
      "        for j in xrange(i+1, len(boundaries)):\n",
      "            if areClustersConnected(boundaries[i], boundaries[j], radius_of_infection):\n",
      "                g_coarse.add_edge(most_connected_gg[i], most_connected_gg[j])\n",
      "                \n",
      "                \n",
      "                \n",
      "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                          ### Functions I developed to make graphs from .tif data. Far more efficient ###\n",
      "                                          ### than my original way, or finding positions, and then making the graph. ###\n",
      "        \n",
      "def getType(x, uses):\n",
      "    \"\"\"Return land use type given integer, x, and dictionary of {type, integer}, uses \"\"\"\n",
      "    for i in uses.items():\n",
      "        if x == i[1]:\n",
      "            return i[0]\n",
      "        \n",
      "        \n",
      "                \n",
      "def getPositions(x, uses, data):\n",
      "    \n",
      "    \"\"\"Return array positions, (row, column), of given data type. \n",
      "\n",
      "       Input is,\n",
      "       x = integer corresponding to data type, uses = dictionary (see above),\n",
      "       data = np.array of import from .tif. \n",
      "        \n",
      "       Output is, \n",
      "       np.array of [[row, colum],...] --> do these need to be conveted to [[x1,y1],...] not sure\n",
      "       how the data is input. \n",
      "        \n",
      "       I should include an error message. \"\"\"\n",
      "\n",
      "\n",
      "    temp = []\n",
      "        \n",
      "    for i in xrange(len(data)):\n",
      "        for j in xrange(len(data[0])):\n",
      "            if data[i][j] == x:\n",
      "                temp.append([i, j])   \n",
      "\n",
      "    return np.array(temp)\n",
      "\n",
      "\n",
      "\n",
      "def compareGraphs(g1, g2):\n",
      "    \n",
      "    \"\"\"#Compares the quantitative properties of two graph. So I can check the coarse graining. \"\"\"\n",
      "\n",
      "    \n",
      "    #Nodes and edges\n",
      "    print 'Graph1: #(Nodes, Edges) = (' + str(len(g1.nodes())) + ', ' + str(len(g1.edges())) + ')'\n",
      "    print 'Graph2: #(Nodes, Edges) = (' + str(len(g2.nodes())) + ', ' + str(len(g2.edges())) + ')'\n",
      "\n",
      "    #Connected Components\n",
      "    #print '\\n#CCs for graph 1: ' + str(len(nx.connected_components(g1)))\n",
      "    #print '#CCs for graph 2: ' + str(len(nx.connected_components(g2)))\n",
      "    \n",
      "    plt.hist([len(i) for i in nx.connected_components(g1)])\n",
      "    plt.hist([len(i) for i in nx.connected_components(g2)])\n",
      "    plt.title('Cluster Size')\n",
      "    plt.xlabel('Cluster Size')\n",
      "    plt.ylabel('#Cluster')\n",
      "    show()\n",
      "    \n",
      "    #Degree Distribution\n",
      "    plt.hist(nx.degree_histogram(g1))\n",
      "    plt.hist(nx.degree_histogram(g2))\n",
      "    plt.title('Degree Distribution' )\n",
      "    plt.xlabel('Degree')\n",
      "    plt.ylabel('#Nodes')\n",
      "    show()\n",
      "    \n",
      "    #Betweeness --- this is by far the most compuationally demanding.\n",
      "    plt.hist(nx.betweenness_centrality(g1, normalized = False).values())\n",
      "    plt.hist(nx.betweenness_centrality(g2, normalized = False).values())\n",
      "    plt.title('Distribution of Betweenness' )\n",
      "    plt.xlabel('Betweenness')\n",
      "    plt.ylabel('#Nodes')\n",
      "    show()        \n",
      "        \n",
      "        \n",
      "                \n",
      "                \n",
      "def connect_four_neighbors(data, i, j, x, g, r):\n",
      "    \n",
      "    \"\"\" This is an auxilliary function for make_graph_from_array defined below.\n",
      "        It check the four neighbouring elements of (i,j) element in .tif data,\n",
      "        and if it has the same landuse type, adds it as a node to graph g, and\n",
      "        connects the two nodes. See make_graph_from_array below for a fuller\n",
      "        explanation\n",
      "    \"\"\"\n",
      "    \n",
      "    try:\n",
      "        if data[i+r][j+r] == x:\n",
      "            if not g.has_node(str(i+r) + ' ' + str(j+r)):\n",
      "                g.add_node(str(i+r) + ' ' + str(j+r))\n",
      "            \n",
      "            g.add_edge(str(i) + ' ' + str(j), str(i+r) + ' ' + str(j+r))\n",
      "    except IndexError:\n",
      "        pass\n",
      "    \n",
      "        \n",
      "    try:\n",
      "        if data[i][j+r] == x:\n",
      "            if not g.has_node(str(i) + ' ' + str(j+r)):\n",
      "                g.add_node(str(i) + ' ' + str(j+r))\n",
      "            \n",
      "            g.add_edge(str(i) + ' ' + str(j), str(i) + ' ' + str(j+r))\n",
      "    except IndexError:\n",
      "        pass\n",
      "        \n",
      "    try:  \n",
      "        if data[i-r][j+r] == x and (i-r) > 0:\n",
      "            if not g.has_node(str(i-r) + ' ' + str(j+r)):\n",
      "                g.add_node(str(i-r) + ' ' + str(j+r))\n",
      "            \n",
      "            g.add_edge(str(i) + ' ' + str(j), str(i-r) + ' ' + str(j+r))\n",
      "    except IndexError:\n",
      "        pass\n",
      "    \n",
      "        \n",
      "    try:\n",
      "        if data[i-r][j] == x and (i-r) > 0:\n",
      "            if not g.has_node(str(i-r) + ' ' + str(j)):\n",
      "                g.add_node(str(i-r) + ' ' + str(j))\n",
      "            \n",
      "            g.add_edge(str(i) + ' ' + str(j), str(i-r) + ' ' + str(j))\n",
      "    except IndexError:\n",
      "        pass\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "def make_hardcut_off_graph_from_array(data, x, r):\n",
      "    \"\"\"Input: data = np.array import from landuse.tif\n",
      "              x = landuse type (integer)\n",
      "              r = coarse graining radius, in units of 'boxes'.\n",
      "   \n",
      "       Output: nx.Graph() with nodes representing landuse type, connected within radius\n",
      "\n",
      "       Outline of Algorithm: check if element (i,j) is of desired type. If yes, add to graph.\n",
      "                             Then find neighbours. Since running from left to right, top to\n",
      "                             bottom, only need to check nodes in 'lower right' quadrant. That\n",
      "                             is, elements {(i, j+1), (i+1, j+1), (i+1, j), (i+1, j-1)} for 'inner'\n",
      "                             layers and '{(i+1, j), (i, j+1)}' for 'outermost' layer. So, 'outermost'\n",
      "                             layer doesn't contain elements touching the middle element 'diagonally',\n",
      "                             that is, at sqrt(2)R from middle element.\n",
      "\n",
      "       Comments: I've used 'try: and except: ' liberally to deal with the edges. This seemed most\n",
      "                 sensible, since the 'edge' is variable (since I'm connecting neighbors at a variable r)\n",
      "                 Maybe generators are better for this?\n",
      "\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "    g = nx.Graph()\n",
      "    \n",
      "    for i in xrange(len(data)):\n",
      "        for j in xrange(len(data[0])):\n",
      "        \n",
      "            if data[i][j] == x:\n",
      "                if not g.has_node(str(i) + ' ' + str(j)):\n",
      "                    g.add_node(str(i) + ' ' + str(j))\n",
      "            \n",
      "    \n",
      "        #Outer Layer has only 2 neighbors\n",
      "                try:\n",
      "                    if data[i+r][j] == x:\n",
      "                        if not g.has_node(str(i+r) + ' ' + str(j)):     #if node isn't already in graph, add it\n",
      "                            g.add_node(str(i+r) + ' ' + str(j))         #connect them.\n",
      "                \n",
      "                        g.add_edge(str(i) + ' ' + str(j), str(i+r) + ' ' + str(j))\n",
      "                except IndexError:\n",
      "                    pass\n",
      "\n",
      "                try:\n",
      "                    if data[i][j+r] == x:\n",
      "                        if not g.has_node(str(i) + ' ' + str(j+r)):\n",
      "                            g.add_node(str(i) + ' ' + str(j+r))\n",
      "                \n",
      "                        g.add_edge(str(i) + ' ' + str(j), str(i) + ' ' + str(j+r))\n",
      "                except IndexError:\n",
      "                    pass\n",
      "        \n",
      "        \n",
      "        #Inner Layers have 4 neighbours.\n",
      "                for n in range(1,r):\n",
      "                    connect_four_neighbors(data, i, j, x, g, n)\n",
      "        \n",
      "    return g \n",
      "\n",
      "\n",
      "\n",
      "def make_graph_with_tail_from_array(data, x, r):\n",
      "    \"\"\" This is the same of make_hard_cut_off_graph_from_array_, except the connectivity r\n",
      "        obeys an exponential decay, with a characteristic distance r_characteristic. This \n",
      "        characteristic distance is taken to be  half of the radius given.\n",
      "\n",
      "    \"\"\"\n",
      "    \n",
      "    g = nx.Graph()\n",
      "    r_characteristic = r/2\n",
      "    \n",
      "    for i in xrange(len(data)):\n",
      "        for j in xrange(len(data[0])):\n",
      "        \n",
      "            if data[i][j] == x:\n",
      "                if not g.has_node(str(i) + ' ' + str(j)):\n",
      "                    g.add_node(str(i) + ' ' + str(j))\n",
      "                    \n",
      "                    r_eff = np.random.exponential(r_characteristic)\n",
      "            \n",
      "    \n",
      "        #Outer Layer has only 2 neighbors\n",
      "                try:\n",
      "                    if data[i+r_eff][j] == x:\n",
      "                        if not g.has_node(str(i+r_eff) + ' ' + str(j)):     #if node isn't already in graph, add it\n",
      "                            g.add_node(str(i+r_eff) + ' ' + str(j))         #connect them.\n",
      "                \n",
      "                        g.add_edge(str(i) + ' ' + str(j), str(i+r_eff) + ' ' + str(j))\n",
      "                except IndexError:\n",
      "                    pass\n",
      "\n",
      "                try:\n",
      "                    if data[i][j+r_eff] == x:\n",
      "                        if not g.has_node(str(i) + ' ' + str(j+r_eff)):\n",
      "                            g.add_node(str(i) + ' ' + str(j+r_eff))\n",
      "                \n",
      "                        g.add_edge(str(i) + ' ' + str(j), str(i) + ' ' + str(j+r_eff))\n",
      "                except IndexError:\n",
      "                    pass\n",
      "        \n",
      "        \n",
      "        #Inner Layers have 4 neighbours.\n",
      "                for n in range(1,int(math.ceil(r_eff))):\n",
      "                    connect_four_neighbors(data, i, j, x, g, n)\n",
      "        \n",
      "    return g \n",
      "\n",
      "\n",
      "\n",
      "def probe_length_scale(data, data_type, r_min, r_max, dr):\n",
      "    \"\"\" \n",
      "        Investigates how the number of connected components changes as a\n",
      "        function of interaction radius. \n",
      "\n",
      "        Input:  data = original data, in np.array form, as imported from .tif\n",
      "                data_type = lanuse type (integer, e.g. winter wheat = 24)\n",
      "                r_min = starting radius of connectivity\n",
      "                r_max = ending radius of connectivity\n",
      "                dr = radius increment\n",
      "        \n",
      "        Output: list of [[radius, #CC's, size of biggest CC],...]\n",
      "    \"\"\"\n",
      "    \n",
      "    cluster_data = []                                              #I use the terms 'cluster' and 'connected component interchangeably'\n",
      "    \n",
      "    for r in range(r_min, r_max + 1, dr):                          #r_max + 1 since python does, range(1,2) = 1, range(1,3) = (1,2), \n",
      "        g_r = fn.make_graph_from_array(data, data_type, r)\n",
      "        CCs = nx.connected_components(g_r)\n",
      "        num_CCs = len(CCs)\n",
      "        size_biggest_CC = len(CCs[0])                              #we know that CC's are arrangest in order of decreasing magnitude,\n",
      "        cluster_data.append([r, num_CCs, size_biggest_CC])         #so the first element is the biggest cluster\n",
      "        \n",
      "    return cluster_data\n",
      "\n",
      "                    \n",
      "                    \n",
      "#------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                   ## MAIN ##\n",
      "    \n",
      "def coarse_grain_graph(data, data_type, radius_of_infection, rips_radius):\n",
      "    \n",
      "    \"\"\"This is my 'first attempt' main function. As I refined the algorithm, it changed,\n",
      "       but I left the original form as comments, for comparison. I'll have to clean it \n",
      "       up properly soon.\n",
      "\n",
      "       Input: data = np.array(.tif data), rips_radius = coarse graining length\n",
      "        \n",
      "       Output: the coarse grained networkx graph\n",
      "\n",
      "    \"\"\"\n",
      "    \n",
      "    #Make and connect graphs\n",
      "    print 'Making Graphs'                                   \n",
      "    \n",
      "    #g = makeGraph(data)                                   #Original graph\n",
      "    #connect(g, radius_of_infection)\n",
      "    \n",
      "    #gg = makeGraph(data)                                  #Rips Reduce graph (coarse graining length)\n",
      "    #connect(gg, rips_radius)\n",
      "\n",
      "    gg = make_graph_from_array(data, data_type, rips_radius)\n",
      "    \n",
      "    print \"Finding CC's\"\n",
      "    connected_components_gg = nx.connected_components(gg) #Find the CC's\n",
      "    most_connected_gg = mostConnectedNodes(gg)            #Find the list of 'biggest' nodes (one for each CC)\n",
      "    \n",
      "    #connected_components_g = nx.connected_components(g)\n",
      "    \n",
      "    #Coarse grained Graph\n",
      "    g_coarse = nx.Graph()\n",
      "    g_coarse.add_nodes_from(most_connected_gg)\n",
      "    \n",
      "    \"\"\" Old Way .\n",
      "    print \"Coarse Graining\"\n",
      "    #Connect coarse grained graph\n",
      "    connectCoarseGraph(most_connected_gg, radius_of_infection, g_coarse)\n",
      "    connectNeighborsNeigbors(most_connected_gg, connected_components_gg, connected_components_g, g_coarse, g) \n",
      "    \"\"\"\n",
      "    \n",
      "    #New way - compute boundary\n",
      "    connectClusters(connected_components_gg, g_coarse, most_connected_gg, radius_of_infection)\n",
      "    \n",
      "    return g_coarse                              # return the coarse grained graph.\n",
      "\n",
      "\n",
      "\n",
      "#----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                 ### Random Walk Functions ###\n",
      "    \n",
      "def random_walk(g, num_steps):\n",
      "    \n",
      "    \"\"\" Perform Random Walk on graph g. Return distance between start and end node \"\"\"\n",
      "    \n",
      "    start = random.choice(g.nodes())\n",
      "    \n",
      "    #Avoid starting in a node with no edges\n",
      "    while len(g[start].keys()) == 0:\n",
      "        start = random.choice(g.nodes())\n",
      "  \n",
      "    current_position = start\n",
      "    \n",
      "    for i in xrange(num_steps):\n",
      "        current_position = random.choice(g[current_position].keys())\n",
      "\n",
      "    end  = current_position\n",
      "\n",
      "    return  dist(start, end)\n",
      "\n",
      "\n",
      "\n",
      "def find_super_node(node, g_r, connected_components_g_r):\n",
      "    \n",
      "    \"\"\" Returns the supernode (which is the node with highest degree, in a given cluster)\n",
      "        of a given node.\n",
      "\n",
      "        Input: g_r = networkx graph connected at coarsening length r\n",
      "               \n",
      "    \"\"\"\n",
      "    \n",
      "    most_connected = [max(g_r.degree(i).iteritems(), key = lambda x:x[1])[0] for i in connected_components_g_r ]\n",
      "    \n",
      "    for i in xrange(len(connected_components_g_r)):\n",
      "        if node in connected_components_g_r[i]:\n",
      "            return most_connected[i]\n",
      "    \n",
      "    \n",
      "\n",
      "def random_walk_non_coarse_grained(g_R, g_r, num_steps, connected_components):\n",
      "    \"\"\" A minor variant on random walk. Changed to measure distance\n",
      "        between nearest supernode of starting and end nodes -- since\n",
      "        the coarse grained graph represents all such nodes by their supernodes.\n",
      "    \"\"\"\n",
      "    \n",
      "        \n",
      "    start = random.choice(g_R.nodes())\n",
      "    \n",
      "    #Avoid starting in a node with no edges\n",
      "    while len(g_R[start].keys()) == 0:\n",
      "        start = random.choice(g_R.nodes())\n",
      "  \n",
      "    current_position = start\n",
      "    \n",
      "    for i in xrange(num_steps):\n",
      "        current_position = random.choice(g_R[current_position].keys())\n",
      "\n",
      "    end  = current_position\n",
      "    return  dist(find_super_node(start, g_r, connected_components), find_super_node(end, g_r, connected_components))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def connect_four_neighbors_tuple(data, i, j, x, g, r):\n",
      "    \n",
      "    \"\"\" This is an auxilliary function for make_graph_from_array defined below.\n",
      "        It check the four neighbouring elements of (i,j) element in .tif data,\n",
      "        and if it has the same landuse type, adds it as a node to graph g, and\n",
      "        connects the two nodes. See make_graph_from_array below for a fuller\n",
      "        explanation\n",
      "    \"\"\"\n",
      "    \n",
      "    try:\n",
      "        if data[i+r][j+r] == x:\n",
      "            if not g.has_node((i+r, j+r)):\n",
      "                g.add_node((i+r, j+r))\n",
      "            \n",
      "            g.add_edge((i, j), (i+r, j+r))\n",
      "    except IndexError:\n",
      "        pass\n",
      "    \n",
      "        \n",
      "    try:\n",
      "        if data[i][j+r] == x:\n",
      "            if not g.has_node((i, j+r)):\n",
      "                g.add_node((i, j+r))\n",
      "            \n",
      "            g.add_edge((i, j), (i, j+r))\n",
      "    except IndexError:\n",
      "        pass\n",
      "        \n",
      "    try:  \n",
      "        if data[i-r][j+r] == x and (i-r) > 0:\n",
      "            if not g.has_node((i-r, j+r)):\n",
      "                g.add_node((i-r, j+r))\n",
      "            \n",
      "            g.add_edge((i, j), (i-r, j-r))\n",
      "    except IndexError:\n",
      "        pass\n",
      "    \n",
      "        \n",
      "    try:\n",
      "        if data[i-r][j] == x and (i-r) > 0:\n",
      "            if not g.has_node((i-r, j)):\n",
      "                g.add_node((i-r, j))\n",
      "            \n",
      "            g.add_edge((i, j), (i-r, j))\n",
      "    except IndexError:\n",
      "        pass\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "def make_hardcut_off_graph_from_array_tuple(data, x, r):\n",
      "    \"\"\"Input: data = np.array import from landuse.tif\n",
      "              x = landuse type (integer)\n",
      "              r = coarse graining radius, in units of 'boxes'.\n",
      "   \n",
      "       Output: nx.Graph() with nodes representing landuse type, connected within radius\n",
      "\n",
      "       Outline of Algorithm: check if element (i,j) is of desired type. If yes, add to graph.\n",
      "                             Then find neighbours. Since running from left to right, top to\n",
      "                             bottom, only need to check nodes in 'lower right' quadrant. That\n",
      "                             is, elements {(i, j+1), (i+1, j+1), (i+1, j), (i+1, j-1)} for 'inner'\n",
      "                             layers and '{(i+1, j), (i, j+1)}' for 'outermost' layer. So, 'outermost'\n",
      "                             layer doesn't contain elements touching the middle element 'diagonally',\n",
      "                             that is, at sqrt(2)R from middle element.\n",
      "\n",
      "       Comments: I've used 'try: and except: ' liberally to deal with the edges. This seemed most\n",
      "                 sensible, since the 'edge' is variable (since I'm connecting neighbors at a variable r)\n",
      "                 Maybe generators are better for this?\n",
      "\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "    g = nx.Graph()\n",
      "    \n",
      "    for i in xrange(len(data)):\n",
      "        for j in xrange(len(data[0])):\n",
      "        \n",
      "            if data[i][j] == x:\n",
      "                if not g.has_node((i, j)):\n",
      "                    g.add_node((i, j))\n",
      "            \n",
      "    \n",
      "        #Outer Layer has only 2 neighbors\n",
      "                try:\n",
      "                    if data[i+r][j] == x:\n",
      "                        if not g.has_node((i+r, j)):     #if node isn't already in graph, add it\n",
      "                            g.add_node((i+r, j))         #connect them.\n",
      "                \n",
      "                        g.add_edge((i,j), (i+r, j))\n",
      "                except IndexError:\n",
      "                    pass\n",
      "\n",
      "                try:\n",
      "                    if data[i][j+r] == x:\n",
      "                        if not g.has_node((i, j+r)):\n",
      "                            g.add_node((i, j+r))\n",
      "                \n",
      "                        g.add_edge((i,j), (i, j+r))\n",
      "                except IndexError:\n",
      "                    pass\n",
      "        \n",
      "        \n",
      "        #Inner Layers have 4 neighbours.\n",
      "                for n in range(1,r):\n",
      "                    connect_four_neighbors_tuple(data, i, j, x, g, n)\n",
      "        \n",
      "    return g "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------------------FUCNTIONS FOR FINDING PURELY ANALYTIC MFPT ---------------------------------------------------------------------\n",
      "def find_numerical_MFPT(start_node, end_node, g, num_of_trials):\n",
      "    \"\"\"Input: g = nx.Graph()  \n",
      "    \"\"\"\n",
      "    \n",
      "    FPT = []                                            #will contain FPT of each trial\n",
      "    for trial in xrange(num_of_trials):\n",
      "        current_node = start_node\n",
      "        FPT_i = 0                                       #FPT per trial\n",
      "        while current_node != end_node:\n",
      "            current_node = random.choice(g[current_node].keys())    #pick a neighbour at random\n",
      "            FPT_i += 1\n",
      "        FPT.append(FPT_i)\n",
      "        \n",
      "    return np.mean(FPT)\n",
      "\n",
      "\n",
      "def K(i, g_trial):\n",
      "    \"\"\"K_i := Sum[A_ij, i]. That is, the number of neighbours\n",
      "       of node K.\n",
      "\n",
      "       This is inefficient -- use nx.neighbors\n",
      "       I'll have to find how nodes are ordered though\n",
      "       that is, which node is (i,j) in adj. Matrix.\n",
      "    \"\"\"\n",
      "    \n",
      "    return len(nx.neighbors(g_trial, g_trial.nodes()[i]))\n",
      "    \n",
      "\n",
      "\n",
      "def find_stationary_probability(node_i, N, g_trial):\n",
      "    \"\"\" P_i(t = inf) = K_i / Sum(K_j) \n",
      "        N := Sum[K_i, i]\n",
      "    \"\"\"\n",
      "    return float(K(node_i, g_trial)) / N \n",
      "\n",
      "\n",
      "\n",
      "def find_zeroth_moment_matrix(A, g_trial, tolerance, max_iteration):\n",
      "    \"\"\" finds the zeroth order moment matrix defined by\n",
      "        R_ij = Sum[P_ij(t) - P_j^inf, {t,0 inf}] as per paper\n",
      "        Since we have an infinte sum, the function has a tolerance.\n",
      "        In my experience, 0.05 is suitable. Convergence is slow however,\n",
      "        so I've included a max iteration also.\n",
      "\n",
      "        Input: A := np.matrix(Adjacency Matrix)\n",
      "    \"\"\"\n",
      "\n",
      "    N = float(np.sum(A))                                 #used in finding stationary probabilities\n",
      "    max_error = 2*tolerance\n",
      "    counter = 0\n",
      "    row = A.shape[0]\n",
      "    prob_matrix = np.matrix(np.identity(row))            #P(t = 0)_ij = Delta_ij \n",
      "    stationary_prob = np.array(np.sum(A,0)[0] / float(np.sum(A)))[0]\n",
      "    zeroth_moment_matrix = np.matrix(np.vstack(tuple([-stationary_prob for i in xrange(len(stationary_prob))])).T)  # - p_j^inf\n",
      "    del stationary_prob\n",
      "    \n",
      "    \n",
      "    while max_error > tolerance and counter < max_iteration:\n",
      "        counter += 1\n",
      "        max_error = 0\n",
      "        err_check = 0.0\n",
      "        zeroth_moment_matrix += prob_matrix\n",
      "        \n",
      "        for j in xrange(row):\n",
      "            err_check = max(prob_matrix[:,j] - find_stationary_probability(j, N, g_trial))\n",
      "            if err_check > max_error:\n",
      "                max_error = err_check\n",
      "            \n",
      "        prob_matrix = (A / np.sum(A, 0))*prob_matrix    \n",
      "            \n",
      "    return zeroth_moment_matrix\n",
      "    \n",
      "\n",
      "def find_MFPT_matrix(A, g_trial, tolerance, max_iteration):\n",
      "    \"\"\" MFPT_matrix is defined by: \n",
      "        <T_ij> := N/K_j                         for i = j\n",
      "        <T_ij> := N/K_j [R_jj^(0) - R_ij^(0)]   for i != j\n",
      "\n",
      "        where N = Sum[K_i, i], K_i = neighbours of node i, \n",
      "        R_ij^(0) = zeroth order moment matrix.\n",
      "        \n",
      "        R_ij^(0) contains an infinite sum, so a tolerance is needed.\n",
      "        Convergence can be slow so a max_iteration is included also\n",
      "        See find_zeroth_order_matrix() for more details.\n",
      "        \n",
      "        Input: A:= np.matrix(Adjacency matrix).\n",
      "    \"\"\"\n",
      "    \n",
      "    MFPT_matrix = find_zeroth_moment_matrix(A, g_trial, tolerance, max_iteration)                                   # will *ultimately* contain MFPT\n",
      "    MFPT_matrix = np.multiply( np.diag(MFPT_matrix) - MFPT_matrix, zip(np.array(float(np.sum(A)) / np.sum(A,0)[0] )[0]))  # T_ij = (R_jj - R_ij) / p_j^inf\n",
      "    np.fill_diagonal(MFPT_matrix, float(np.sum(A)) / np.sum(A,0)[0] )                                               # T_ii = 1 / p_i^inf\n",
      "    return MFPT_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#---------------------------------------------------- COARSE GRAINING WITH RING STRUCTURE ------------------------------------------------------------------\n",
      "\"\"\" Based on \"Ring structures and mean first passage time in networks\"\n",
      "    by Andrea Baronchelli and Vittorio Loreto\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def find_analytical_ring_MFPT(start_node, g, tolerance, max_iteration):\n",
      "    \"\"\" Input: start_node in 'x y' format. g = nx.graph. Contains infinite\n",
      "        sum, so tolerance and max interation are included. A tolerance of 0.01\n",
      "        produced satisfactory convergence for me. (Although this obviously depended\n",
      "        on the trial graph I was using)\n",
      "    \"\"\"\n",
      "                \n",
      "    rings = make_rings(g, start_node)\n",
      "    diag_m = find_diagonal_m(rings, g)\n",
      "    off_diag_m = find_off_diagonal_m(rings, g)\n",
      "    B = make_B(diag_m, off_diag_m)\n",
      "    number_nodes = len(g.nodes())              #can delete g hereafter\n",
      "    \n",
      "    #del diag_m      # hardly makes a difference?\n",
      "    #del off_diag_m  \n",
      "    \n",
      "    term = 1.0\n",
      "    error = 0.0\n",
      "    total = 0.0\n",
      "    counter = 0\n",
      "    time = 1\n",
      "    \n",
      "    while term > tolerance and counter < max_iteration:\n",
      "        term = time*find_first_passage_probability(time, rings, B, number_nodes)\n",
      "        total += term\n",
      "        time += 1\n",
      "        counter += 1\n",
      "        \n",
      "    return total\n",
      "\n",
      "\n",
      "\n",
      "def make_rings(g, start_node):\n",
      "    \"\"\" Find the rings around a given node, composing a graph. A ring of length l\n",
      "        centered as node i, is defined as the set of nodes that are 'l' edges away\n",
      "        from the node i.\n",
      "\n",
      "        Input: nx.Graph, and a node in this graph.\n",
      "        Output: a dictionary, {L, ring of length L}.\n",
      "    \"\"\"\n",
      "\n",
      "    rings = {}\n",
      "\n",
      "    for i in g.nodes():\n",
      "        length = nx.shortest_path_length(g, start_node, i)\n",
      "        \n",
      "        #If ring hasn't been created already, do so\n",
      "        if length not in rings.keys():\n",
      "            rings[length] = []\n",
      "        \n",
      "        #add node to relevant ring\n",
      "        rings[length].append(i)\n",
      "        \n",
      "    return rings\n",
      "\n",
      "\n",
      "\n",
      "def find_diagonal_m(rings, g):\n",
      "    \"\"\" Finds the diagonal element of the matrix 'm',\n",
      "        as defined in the coarse graining procedure.\n",
      "        (An intermediary to the matrix B)\n",
      "\n",
      "        Input: nx.Graph, and dictionary of {L, ring of length L}\n",
      "    \"\"\"\n",
      "    \n",
      "    diag_m = []\n",
      "    \n",
      "    for ring in rings.values():\n",
      "        degree_distribution_per_ring = []\n",
      "        \n",
      "        for node in ring:\n",
      "            internal_degree_node_i = 0.0\n",
      "            \n",
      "            for neighbour in g[node]:     #all of node's neighbours\n",
      "                if neighbour in ring:\n",
      "                    internal_degree_node_i += 1\n",
      "                \n",
      "            degree_distribution_per_ring.append(internal_degree_node_i)\n",
      "            \n",
      "        diag_m.append(sum(degree_distribution_per_ring))\n",
      "            \n",
      "    return diag_m\n",
      "\n",
      "\n",
      "\n",
      "def find_off_diagonal_m(rings, g):\n",
      "    \"\"\" Intermediary Function to find matrix B \"\"\"\n",
      "    \n",
      "    off_diag_m = []\n",
      "    \n",
      "    for k in range(len(rings)-1):\n",
      "        ring1 = rings.values()[k]\n",
      "        ring2 = rings.values()[k+1]\n",
      "        degree_distribution_per_ring = []\n",
      "        \n",
      "        for node in ring1:\n",
      "            external_degree_node_i = 0.0\n",
      "            \n",
      "            for neighbour in g[node]:\n",
      "                if neighbour in ring2:\n",
      "                    external_degree_node_i += 1\n",
      "            \n",
      "            degree_distribution_per_ring.append(external_degree_node_i)\n",
      "        \n",
      "        off_diag_m.append(sum(degree_distribution_per_ring))\n",
      "        \n",
      "    return off_diag_m\n",
      "\n",
      "\n",
      "\n",
      "def make_B(diag_m, off_diag_m):\n",
      "    \"\"\"Constructs the matrix B, defined in paper, which represents\n",
      "       the transition matrix for the ring process: B_ij(t) = prob.\n",
      "       to go from ring_i -> ring_j in t steps. We are interested in \n",
      "       FIRST passage time, so this is an absorbing random walk, so\n",
      "       the first row = 0 (can't leave ring 0).\n",
      "\n",
      "       This is an intermediate function function to find MFPT. \n",
      "\n",
      "    \"\"\"\n",
      "    \n",
      "    main_diagonal = [(2*diag_m[0]) / (2*diag_m[0] + off_diag_m[0])]   \n",
      "    super_diagonal = [off_diag_m[0] / (2*diag_m[0] + off_diag_m[0])]\n",
      "    sub_diagonal = []\n",
      "    \n",
      "    #iterate, avoiding the boundaries\n",
      "    for i in range(1,len(diag_m)-1):\n",
      "        main_diagonal.append((2*diag_m[i]) / (2*diag_m[i] + off_diag_m[i-1] + off_diag_m[i]))\n",
      "        super_diagonal.append(off_diag_m[i] / (2*diag_m[i] + off_diag_m[i-1] + off_diag_m[i]))\n",
      "        sub_diagonal.append(off_diag_m[i-1] / ((2*diag_m[i] + off_diag_m[i-1] + off_diag_m[i])))\n",
      "        \n",
      "    n = len(diag_m) - 1\n",
      "    main_diagonal.append(2*diag_m[n] / (2*diag_m[n] + off_diag_m[n-1]))\n",
      "    sub_diagonal.append(off_diag_m[n-1] / (2*diag_m[n] + off_diag_m[n-1]))\n",
      "        \n",
      "        \n",
      "    \n",
      "    #These are to make the sparse matrix\n",
      "    temp1 = np.array([main_diagonal, [1] + super_diagonal, sub_diagonal + [1]])   #the three diagonals; prepend/appended 1 to get positioning right\n",
      "    temp2 = np.array([0, 1, -1])                                                  #main, super, and sub, diagonals\n",
      "   \n",
      "    B = ss.spdiags(temp1, temp2, len(main_diagonal), len(main_diagonal)).todense()\n",
      "    \n",
      "    #Make top row zero, since we want FIRST passage time\n",
      "    B[0,0], B[0,1] = 0,0\n",
      "    \n",
      "    return B\n",
      "\n",
      "\n",
      "\n",
      "def find_first_passage_probability(t, rings, B, number_nodes):\n",
      "    \"\"\" Defined in paper. Intermediate function to find MFPT \"\"\"\n",
      "    \n",
      "    temp = 0.0\n",
      "    for i in range(1,len(rings.values())):         #don't include ring_0\n",
      "        temp += float((len(rings.values()[i]))) / (number_nodes -1) * (B**t)[i,0]\n",
      " \n",
      "    return temp\n",
      "\n",
      "\n",
      "\n",
      "def find_numerical_ring_MFPT(start_node, end_node, g, num_trials):\n",
      "    \"\"\" Input: start_node, end_node in (x, y) format. g = nx.Graph()\n",
      "        Output: [mean number of rings crossed, mean number of steps taken]\n",
      "\n",
      "        Assumes graph is fully connected.\n",
      "    \"\"\"\n",
      "        \n",
      "    steps = []\n",
      "    trial = 0\n",
      "\n",
      "    while trial <= num_trials:\n",
      "        current_node = start_node\n",
      "        steps_per_trial = 0\n",
      "            \n",
      "        while current_node != end_node: \n",
      "            current_node = random.choice(g[current_node].keys())\n",
      "            steps_per_trial += 1\n",
      "            \n",
      "        steps.append(steps_per_trial)\n",
      "        trial += 1\n",
      "            \n",
      "    return np.mean(steps)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}